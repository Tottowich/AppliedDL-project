{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available (using '/physical_device:GPU:0'). Training will be lightning fast!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "# random.seed(2023)\n",
    "import argparse\n",
    "from argparse import BooleanOptionalAction\n",
    "use_fp16 = False\n",
    "use_wandb = False\n",
    "use_neptune = False\n",
    "use_tensorboard = True\n",
    "import numpy as np\n",
    "\n",
    "# np.random.seed(2023)  # Set seed for reproducibility\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tf.random.set_seed(2023)\n",
    "tf.config.run_functions_eagerly(True)\n",
    "keras.backend.clear_session()\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, \\\n",
    "    TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "\n",
    "from archs.segmentation.unet import build_unet\n",
    "from data.data_generator import loaders\n",
    "from utils.helper import create_dirs, write_setup, gpu_setup\n",
    "from utils.loss import FocalDiceLoss, dice_coef, dice_coef_loss\n",
    "from utils.visualizations import plot_sample\n",
    "# from archs.hpo import hyperparameter_build\n",
    "gpu_setup(fp16=use_fp16)\n",
    "# Set up directories and variables\n",
    "array_labels = ['t1', 't1ce', 't2', 'flair', 'mask']\n",
    "name = \"tumor-segmentation-keras-tuner\"\n",
    "gen_dir = \"/home/thjo/Datasets/Brats/\"\n",
    "model_dir = \"./search/\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "max_iter = 40\n",
    "batch_size = 256\n",
    "# Initialize WandB if specified\n",
    "if use_wandb:\n",
    "    print(f\"Initalizing wandb for project {name}\")\n",
    "    import wandb\n",
    "    from wandb.keras import WandbCallback\n",
    "    wandb.init(project=name)\n",
    "elif use_neptune:\n",
    "    print(f\"Initalizing neptune for project {name}\")\n",
    "    import neptune.legacy as neptune\n",
    "    import neptunecontrib.monitoring.kerastuner as npt_utils\n",
    "    neptune.init(project_qualified_name=name)\n",
    "    neptune.create_experiment(name=name, params={\"fp16\": use_fp16})\n",
    "    neptune.create_experiment('bayesian-sweep')\n",
    "elif use_tensorboard:\n",
    "    log_dir = os.path.join(model_dir, name, \"logs\")\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the tuner\n",
    "input_shape = (64, 64, len(array_labels) - 1)\n",
    "num_classes = 1\n",
    "max_depth = 6\n",
    "kernel_size = 3\n",
    "strides = 1\n",
    "activations = [\"relu\", \"gelu\", \"selu\"]\n",
    "padding  = \"same\"\n",
    "output_activation = \"sigmoid\"\n",
    "decoder_types = [\"concat\", \"add\"]\n",
    "upsample_types = [\"transposed\", \"bilinear\"]\n",
    "losses = {\"focal\": FocalDiceLoss, \"dice\": dice_coef_loss}\n",
    "losses_keys = list(losses.keys())\n",
    "filter_step = 16\n",
    "depth = 5\n",
    "def hyperparameter_build(kt:kt.HyperParameters):\n",
    "    depth = kt.Int('depth', min_value=4, max_value=max_depth, step=1)\n",
    "    filters = [kt.Int(f'filter_{i}', min_value=2**(i+3), max_value=2**(i+5), step=filter_step) for i in range(depth)]\n",
    "    activation = kt.Choice('activation', values=activations)\n",
    "    depth_encoder = [kt.Int(f'depth_encoder_{i}', min_value=1, max_value=i+1, step=1) for i in range(depth)]\n",
    "    depth_decoder = [kt.Int(f'depth_decoder_{i}', min_value=1, max_value=i+1, step=1) for i in range(depth)]\n",
    "    drop_rate_encoder = [kt.Float(f'drop_rate_encoder_{i}', min_value=0.0, max_value=0.3, step=0.1, default=0.05) for i in range(depth)]\n",
    "    drop_rate_decoder = [kt.Float(f'drop_rate_decoder_{i}', min_value=0.0, max_value=0.3, step=0.1, default=0.05) for i in range(depth)]\n",
    "    output_depth = kt.Int('output_depth', min_value=1, max_value=6, step=1)\n",
    "    # decoder_type = kt.Choice('decoder_type', values=decoder_types)\n",
    "    upsample_type = kt.Choice('upsample_type', values=upsample_types)\n",
    "    unet = build_unet(\n",
    "            input_shape=input_shape,\n",
    "            num_classes=num_classes,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            activation=activation,\n",
    "            depth_encoder=depth_encoder,\n",
    "            depth_decoder=depth_decoder,\n",
    "            drop_rate_encoder=drop_rate_encoder,\n",
    "            drop_rate_decoder=drop_rate_decoder,\n",
    "            output_depth=output_depth,\n",
    "            output_activation=output_activation,\n",
    "            decoder_type=\"concat\",\n",
    "            upsample_type=upsample_type,\n",
    "        )\n",
    "    loss = losses[kt.Choice('loss', values=losses_keys)]\n",
    "    if loss == FocalDiceLoss:\n",
    "        gamma = kt.Float('gamma', min_value=0.0, max_value=5.0, step=0.25)\n",
    "        w_focal = kt.Float('w_focal', min_value=0.0, max_value=1, step=0.05)\n",
    "        w_dice = 1 - w_focal\n",
    "        loss = loss(w_focal=w_focal,w_dice=w_dice,gamma=gamma)\n",
    "    learning_rate = kt.Float('learning_rate', min_value=5e-4, max_value=5e-2, default=1e-2, step=1e-3)\n",
    "    weight_decay = kt.Float('weight_decay', min_value=1e-6, max_value=5e-2, default=1e-3, step=1e-3)\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=weight_decay)\n",
    "    unet.compile(optimizer=optimizer, loss=loss, metrics=[dice_coef])\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if use_wandb:\n",
    "    logger = WandbCallback()\n",
    "elif use_neptune:\n",
    "    logger = npt_utils.NeptuneLogger()\n",
    "else:\n",
    "    logger = None\n",
    "monitor = \"val_dice_coef\"\n",
    "direction = \"max\"\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hyperparameter_build,\n",
    "    objective=kt.Objective(monitor, direction=direction),\n",
    "    max_trials=60,\n",
    "    directory=model_dir,\n",
    "    project_name=name,\n",
    "    overwrite=True,\n",
    "    seed=2023,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if use_wandb:\n",
    "    callbacks.append(WandbCallback())\n",
    "elif use_neptune:\n",
    "    callbacks.append(npt_utils.NeptuneCallback(log_models=True))\n",
    "elif use_tensorboard:\n",
    "    callbacks.append(TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
    "early_stopping = EarlyStopping(monitor=monitor, patience=5, verbose=1, mode=direction, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2, verbose=1, mode=direction)\n",
    "callbacks.append(early_stopping)\n",
    "callbacks.append(reduce_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen_train, gen_val, gen_test, _ = loaders(gen_dir=gen_dir, batch_size=batch_size, augment=True, array_labels=array_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 00m 12s]\n",
      "\n",
      "Best val_dice_coef So Far: 0.8717613816261292\n",
      "Total elapsed time: 01h 23m 04s\n",
      "\n",
      "Search: Running Trial #29\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "5                 |5                 |depth\n",
      "24                |24                |filter_0\n",
      "32                |16                |filter_1\n",
      "64                |96                |filter_2\n",
      "80                |80                |filter_3\n",
      "gelu              |relu              |activation\n",
      "1                 |1                 |depth_encoder_0\n",
      "1                 |2                 |depth_encoder_1\n",
      "2                 |1                 |depth_encoder_2\n",
      "4                 |3                 |depth_encoder_3\n",
      "1                 |1                 |depth_decoder_0\n",
      "2                 |2                 |depth_decoder_1\n",
      "1                 |3                 |depth_decoder_2\n",
      "1                 |2                 |depth_decoder_3\n",
      "0                 |0                 |drop_rate_encoder_0\n",
      "0.2               |0.2               |drop_rate_encoder_1\n",
      "0.1               |0                 |drop_rate_encoder_2\n",
      "0.1               |0                 |drop_rate_encoder_3\n",
      "0.2               |0.1               |drop_rate_decoder_0\n",
      "0.2               |0.1               |drop_rate_decoder_1\n",
      "0.2               |0                 |drop_rate_decoder_2\n",
      "0.1               |0.2               |drop_rate_decoder_3\n",
      "4                 |3                 |output_depth\n",
      "bilinear          |transposed        |upsample_type\n",
      "focal             |dice              |loss\n",
      "0.5               |3                 |gamma\n",
      "0.05              |0.55              |w_focal\n",
      "0.0485            |0.0395            |learning_rate\n",
      "0.001001          |0.022001          |weight_decay\n",
      "144               |448               |filter_4\n",
      "5                 |1                 |depth_encoder_4\n",
      "4                 |2                 |depth_decoder_4\n",
      "0.1               |0.1               |drop_rate_encoder_4\n",
      "0                 |0.2               |drop_rate_decoder_4\n",
      "880               |560               |filter_5\n",
      "6                 |2                 |depth_encoder_5\n",
      "1                 |2                 |depth_decoder_5\n",
      "0.2               |0                 |drop_rate_encoder_5\n",
      "0.2               |0                 |drop_rate_decoder_5\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 214, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py\", line 144, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/thjo/Code/School/AppliedDL-project/archs/segmentation/model_utils.py\", line 73, in call\n",
      "    y = self.activation(self.seq(x)+self.skip(residual))\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'batch_normalization_44' (type BatchNormalization).\n",
      "\n",
      "{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[256,24,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNormV3]\n",
      "\n",
      "Call arguments received by layer 'batch_normalization_44' (type BatchNormalization):\n",
      "  • inputs=tf.Tensor(shape=(256, 32, 32, 24), dtype=float32)\n",
      "  • training=True\n",
      "  • mask=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 214, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py\", line 144, in fit\n    return model.fit(*args, **kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/thjo/Code/School/AppliedDL-project/archs/segmentation/model_utils.py\", line 73, in call\n    y = self.activation(self.seq(x)+self.skip(residual))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'batch_normalization_44' (type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[256,24,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNormV3]\n\nCall arguments received by layer 'batch_normalization_44' (type BatchNormalization):\n  • inputs=tf.Tensor(shape=(256, 32, 32, 24), dtype=float32)\n  • training=True\n  • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(gen_train, epochs\u001b[39m=\u001b[39;49mmax_iter, validation_data\u001b[39m=\u001b[39;49mgen_val, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m      2\u001b[0m tuner\u001b[39m.\u001b[39mresults_summary()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:231\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_run_and_update_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_trial_end(trial)\n\u001b[1;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py:335\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_trial_end\u001b[39m(\u001b[39mself\u001b[39m, trial):\n\u001b[1;32m    330\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moracle\u001b[39m.\u001b[39;49mend_trial(trial)\n\u001b[1;32m    336\u001b[0m     \u001b[39m# Display needs the updated trial scored by the Oracle.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_display\u001b[39m.\u001b[39mon_trial_end(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_trial(trial\u001b[39m.\u001b[39mtrial_id))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:107\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     LOCKS[oracle]\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    106\u001b[0m     THREADS[oracle] \u001b[39m=\u001b[39m thread_name\n\u001b[0;32m--> 107\u001b[0m ret_val \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m need_acquire:\n\u001b[1;32m    109\u001b[0m     THREADS[oracle] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:434\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry(trial):\n\u001b[1;32m    433\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend_order\u001b[39m.\u001b[39mappend(trial\u001b[39m.\u001b[39mtrial_id)\n\u001b[0;32m--> 434\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_consecutive_failures()\n\u001b[1;32m    436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/oracle.py:386\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m     consecutive_failures \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m consecutive_failures \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of consecutive failures excceeded the limit \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_consecutive_failed_trials\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m+\u001b[39m trial\u001b[39m.\u001b[39mmessage\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures excceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/tuner.py\", line 214, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras_tuner/engine/hypermodel.py\", line 144, in fit\n    return model.fit(*args, **kwargs)\n  File \"/home/thjo/miniconda3/envs/tf2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/thjo/Code/School/AppliedDL-project/archs/segmentation/model_utils.py\", line 73, in call\n    y = self.activation(self.seq(x)+self.skip(residual))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'batch_normalization_44' (type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[256,24,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNormV3]\n\nCall arguments received by layer 'batch_normalization_44' (type BatchNormalization):\n  • inputs=tf.Tensor(shape=(256, 32, 32, 24), dtype=float32)\n  • training=True\n  • mask=None\n"
     ]
    }
   ],
   "source": [
    "tuner.search(gen_train, epochs=max_iter, validation_data=gen_val, callbacks=callbacks)\n",
    "tuner.results_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
